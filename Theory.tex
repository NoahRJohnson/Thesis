\chapter{Theory}

EKFs are important to this thesis

Robots estimate their environment stochastically, and so probability theory is vital to understanding their inner workings. First we will review the necessary theory, and then we will examine a class of algorithms for recursive state estimation. For a more in-depth look at this material, see Probabilistic Robotics \cite{probabilisticRobotics}.

\section{Probability Background}
Discrete random variables have a finite output space of possible values that may be observed. Let X be a random variable, then we define the probability that we observe value x from X as \( p(X = x) \equiv p(x)\). Since x is arbitrary, this defines a probability distribution. For every random variable X, we have
\begin{equation*}
\sum\limits_{x \in X} p(x) = 1
\end{equation*}
Given two more random variables Y and Z, we'll define the joint distribution \(p(X=x\ \textrm{and}\ Y=y\ \textrm{and}\ Z=z) \equiv p(x,y,z)\), and the conditional probability \(p(X=x\ \textrm{given that}\ Y=y\ \textrm{and}\ Z=z) \equiv p(x \mathbin{\vert} y,z)\). The conditional probability is defined to be
\begin{equation} \label{eqCondProb}
p(x \mathbin{\vert} y,z) = \frac{p(x,y,z)}{p(y,z)}
\end{equation}

The \textit{Law of Total Probability} states that \(p(x) = \sum\limits_{y \in Y} p(x,y)\). Extending this law to use a third random variable Z, and incorporating the definition of conditional probability, we end up with the following equation:
\begin{equation} \label{eqTotalProb}
p(x \mathbin{\vert} z) = \sum\limits_{y \in Y} p(x,y,z)
= \sum\limits_{y \in Y} p(y,z)p(x \mathbin{\vert} y,z)
\end{equation}

Lastly, we can use equation \refeq{eqCondProb} to derive a version of Bayes' Theorem. 
\begin{equation} \label{eqBayesThm}
p(x \mathbin{\vert} y,z) = \frac{p(x,y,z)}{p(y,z)} = \frac{p(y,x,z)}{p(x,z)} * \frac{p(x,z)}{p(y,z)} = \frac{p(y \mathbin{\vert} x,z)p(x,z)}{p(y \mathbin{\vert} z)}
\end{equation}
In the future this will prove to be a useful tool to compute a posterior probability distribution \(p(x \mathbin{\vert} y)\) from the inverse conditional probability \(p(y \mathbin{\vert} x)\) and the prior probability distribution \(p(x)\).

\section{Bayes Filter}
\subsection{Scenario}
Consider the general case of a robot which uses sensors to gather information about its environment. These sensors provide readings at discrete time steps \(t=0,1,2,...\). Some amount of noise is associated with each of these readings. At each time step \(t\), the robot may execute commands to affect its environment, and wishes to know its current state. \cite{probabilisticRobotics}

Let's encode the robot's current state at time \(t\) in the vector \(x_t\). Similarly, \(z_t\) will represent a sensor measurement at time \(t\), and \(u_t\) will represent the commands issued by the robot at time \(t\). For each of these vectors we will use the notation \(z_{1:t} = z_1, z_2, ..., z_t\). \cite{probabilisticRobotics}

The robot only has access to data in the form of \(z_t\) and \(u_t\). Thus it cannot ever have perfect knowledge of its state \(x_t\). It will have to make do by storing a probability distribution assigning a probability to every possible realization of \(x_t\). This posterior probability distribution will represent the robot's belief in its current state, and should be conditioned on all available data. Thus we'll define the robot's belief distribution to be \cite{probabilisticRobotics}:
\begin{equation} \label{eqBel}
bel(x_t) = p(x_t \mathbin{\vert} z_{1:t}, u_{1:t})
\end{equation}

\subsection{Derivation}

We can use equation \ref{eqBayesThm} to rewrite \(bel(x_t)\):
\begin{equation*}
bel(x_t) = p(x_t \mathbin{\vert} z_{1:t}, u_{1:t}) = \frac{p(z_t \mathbin{\vert} x_t, z_{1:t-1}, u_{1:t})p(x_t \mathbin{\vert} z_{1:t-1}, u_{1:t})}{p(z_t \mathbin{\vert} z_{1:t-1}, u_{1:t})}
\end{equation*}

In order to simplify \(p(z_t \mathbin{\vert} x_t, z_{1:t-1}, u_{1:t})\), we'll have to make an important assumption. We'll assume that the state \(x_t\) satisfies the Markov property, that is, \(x_t\) perfectly encapsulates all prior information. Thus if \(x_t\) is known, then \(z_{1:t}\) and \(u_{1:t}\) are redundant. This assumption lets us remove consideration of past sensor measurements and commands, and to rewrite the belief distribution as:
\begin{equation*}
bel(x_t) = \frac{p(z_t \mathbin{\vert} x_t)p(x_t \mathbin{\vert} z_{1:t-1}, u_{1:t})}{p(z_t \mathbin{\vert} z_{1:t-1}, u_{1:t})}
\end{equation*}

Notice that \(p(z_t \mathbin{\vert} z_{1:t-1}, u_{1:t})\) is a constant with respect to \(x_t\). Thus it makes sense to let \(\eta = (p(z_t \mathbin{\vert} z_{1:t-1}, u_{1:t}))^{-1}\) and rewrite the belief distribution as:
\begin{equation*}
bel(x_t) = \eta p(z_t \mathbin{\vert} x_t) p(x_t \mathbin{\vert} z_{1:t-1}, u_{1:t})
\end{equation*}

Now we are left with two distributions of interest. Looking closely one may notice that \(p(x_t \mathbin{\vert} z_{1:t-1}, u_{1:t})\) is simply our original belief distribution, equation \ref{eqBel}, but not conditioned on the most recent sensor measurement, \(z_t\). Let us refer to this distribution as \(\overline{bel}(x_t)\), and break it down further using equation \ref{eqTotalProb} and our Markov assumption \cite{probabilisticRobotics}:
\begin{multline*}
\overline{bel}(x_t) = p(x_t \mathbin{\vert} z_{1:t-1}, u_{1:t}) \\
= \sum\limits_{x_{t-1}} p(x_t \mathbin{\vert} x_{t-1}, z_{1:t-1}, u_{1:t}) p(x_{t-1} \mathbin{\vert} z_{1:t-1}, u_{1:t}) \\
= \sum\limits_{x_{t-1}} p(x_t \mathbin{\vert} x_{t-1}, u_t) p(x_{t-1} \mathbin{\vert} z_{1:t-1}, u_{1:t}) \\
= \sum\limits_{x_{t-1}} p(x_t \mathbin{\vert} x_{t-1}, u_t) bel(x_{t-1})
\end{multline*}

We have arrived at a recursive definition of \(bel(x_t)\) with respect to \(bel(x_{t-1})\)! As long as \(p(x_t \mathbin{\vert} x_{t-1}, u_t)\) and \(p(z_t \mathbin{\vert} x_t)\) are known, we can recursively calculate \(bel(x_t)\).

\(p(x_t \mathbin{\vert} x_{t-1}, u_t)\) defines a stochastic model for the robot's state, defining how the robot's state will evolve over time based upon what commands it issues. This probability distribution is known as the \textit{state transition probability}. \cite{probabilisticRobotics}

\(p(z_t \mathbin{\vert} x_t)\) also defines a stochastic model, modeling the sensor measurements \(z_t\) as noisy projections of the robot's environment. This distribution will be referred to as the \textit{measurement probability}. \cite{probabilisticRobotics}

Once we have models for both the \textit{state transition probability} and \textit{measurement probability}, we can finally construct the algorithm known as Bayes' Filter \cite{probabilisticRobotics}:

\begin{algorithm} 
\caption{Bayes Filter} 
\label{BayesFilter}
\begin{algorithmic}[1]
\Function{BayesFilterIterate}{ $bel(x_{t-1})$, $u_t$, $z_t$ }
	\For{each possible state \(x_t^* \in x_t\) }
	\State{\(\overline{bel}(x_t^*) = \sum\limits_{x_{t-1}^* \in x_{t-1}} p(x_t^* \mathbin{\vert} x_{t-1}^*, u_t) bel(x_{t-1}^*) \)}
	\State{\(bel(x_t^*) = \eta p(z_t \mathbin{\vert} x_t^*) \overline{bel}(x_t^*) \)}
	\EndFor
	\State{Set \(\sum\limits_{x_t^* \in x_t} bel(x_t^*) = 1\), and solve for \(\eta \)}
	\State{Use \(\eta\) to compute \(bel(x_t)\)}
	\State \Return{\(bel(x_t)\)}
\EndFunction
\end{algorithmic}
\end{algorithm}

%\subsection{Example}




%\section{Kalman Filter}


\section{Extended Kalman Filter} \label{sectionEKF}


\cite{ekfTut}
\newcommand{\bel}{\mathrm{bel}}

\chapter{Theory}

This chapter will dive into the mathematical theory behind the recursive state estimator used in this project, the Extended Kalman Filter. Steps have been taken to carefully explain every computation made for the less mathematically inclined. This section is largely based on chapters 1-3 of the book Probabilistic Robotics \cite{probabilisticRobotics}, which the interested reader should view for a broader look at the same material. 

\section{Probability Theory Background} \label{sectionProbTheory}
To gather information about their environment, robots use sensor data. This data always has some amount of random noise associated with it. Thus probability theory is vital to creating models which incorporate  this uncertainty. Here we will review some probability theory results which will be needed in this chapter.

% technically p(X=x) = 0 for continuous PDFs, but we're working with finite precision computers so who cares. It's easier to think of this way.
Random variables are objects from which specific numbers may be observed. Let X be a continuous random variable - that is, observations of X are real numbers. We will represent the probability of observing any particular real number x from X as \( p(X = x) \equiv p(x)\). This defines a function \(p: \mathbb{R} \to [0,1]\) representing the distribution of probabilities across the real numbers. Throughout this chapter, we will refer to such functions as Probability Density Functions (PDFs). Note the basic result that for every PDF \(p\) constructed from a random variable X, \(\int_{-\infty}^{+\infty} p(x)dx = 1\), which is to say that the observed value from X will be some real number with 100\% certainty.

An important tool for describing PDFs is their expectation. \(E[X]\) is the expected value or mean of X. Note that the expectation operator is linear: \(E[\alpha X + \beta Y] = \alpha E[X] + \beta E[Y]\).

For two random variables X and Y, we will define their covariance to be
\[
cov(X,Y) = E[(X - E[X])(Y - E[Y])]
\] This gives a measure of the co-dependence between X and Y.

The same concepts may be scaled up to random vectors, which are \(n\)-dimensional collections of random variables. Then \(X = (X_1,X_2,...,X_n)^T\), and \(p:\mathbb{R}^n \to [0,1]\) still defines a PDF. The expectation becomes \[
E[X] = \begin{pmatrix}
E[X_1] \\
\vdots \\
E[X_n]
\end{pmatrix} 
\]

and we can define a covariance matrix \(\Sigma\) for the random vector:
\[
\Sigma = E[(X - E[X])(X - E[X])^T] = \begin{pmatrix}
cov(X_1,X_1)  \dots cov(X_1,X_n) \\
\vdots \ddots \vdots \\
cov(X_n,X_1)  \dots cov(X_n,X_n)
\end{pmatrix} 
\]


Now consider three random variables: X, Y, and Z. Define their joint distribution \(p(X=x\ \textrm{and}\ Y=y\ \textrm{and}\ Z=z) \equiv p(x,y,z)\), and the conditional probability \(p(X=x\ \textrm{given that}\ Y=y\ \textrm{and}\ Z=z) \equiv p(x \mathbin{\vert} y,z)\). The conditional probability is given to be
\begin{equation} \label{eqCondProb}
p(x \mathbin{\vert} y,z) = \frac{p(x,y,z)}{p(y,z)}
\end{equation}

The \textit{Law of Total Probability} states that \(p(x) = \int p(x,y)dy\). Conditioning this law on a third random variable Z, and incorporating the definition of conditional probability (Eq. \ref{eqCondProb}), we end up with the following version of this law:
\begin{multline} \label{eqTotalProb}
p(x \mathbin{\vert} z) = \int p(x,y \mathbin{\vert} z)dy = \int \frac{p(x,y,z)}{p(z)}dy  \\
= \int \frac{p(x,y,z)}{p(y,z)} * \frac{p(y,z)}{p(z)}dy = \int p(x \mathbin{\vert} y,z) p(y\mathbin{\vert}z)dy \hfill
\end{multline}

In a similar fashion, we can use the definition of conditional probability (Eq. \ref{eqCondProb}) to derive a version of Bayes' Rule. 
\begin{multline} \label{eqBayesThm}
p(x \mathbin{\vert} y,z) = \frac{p(x,y,z)}{p(y,z)} = \frac{p(y,x,z)}{p(x,z)} * \frac{p(x,z)}{p(z)} * \frac{p(z)}{p(y,z)} \\
= \frac{p(y \mathbin{\vert} x,z)p(x\mathbin{\vert}z)}{p(y \mathbin{\vert} z)} \hfill
\end{multline}
In the future these will prove to be useful tools to update a PDF with new information. When presented with new information that  \(Y = y\), one may use Bayes' Rule to transform the current PDF \(p(x \mathbin{\vert}z)\) into the new posterior PDF \(p(x \mathbin{\vert} y,z)\). %from the inverse conditional probability \(p(y \mathbin{\vert} x)\) and the prior PDF \(p(x)\).

\section{Bayes Filter}
\subsection{Scenario}
Now, back to the inner workings of our rover. Consider the general case of a robot which uses some array of sensors to gather information about its environment. The robot wishes to use these measurements to estimate its current state, where the state is a collection of variables of interest (e.g. position, orientation, velocity, acceleration, etc.).

For simplicity, let us consider the rover to operate in discrete time steps (\(t=0,1,2,...\)). We can encode all state variables of interest in the vector \(x_t = (x_{1_t}, x_{2_t}, ... , x_{n_t})^T\). Similarly, let \(z_t = (z_{1_t}, z_{2_t}, ... , z_{k_t})^T\) represent a sensor measurement at time \(t\). For both of these vectors we will use the compact notation \(a_{1:t} = a_1, a_2, ..., a_t\) to denote the set of all vectors up to and including time \(t\).

The robot wishes to know \(x_t\), however this true state is hidden. It only has access to raw sensor data in the form of \(z_{1:t}\). Sensor measurements always contain some level of noise due to ambient interference, no matter how precise the hardware. Thus each of these measurements will have some amount of error, and the robot will need to account for this in its state estimate. The robot can do this by constructing a PDF assigning a probability to every possible realization of \(x_t\). This PDF will represent the robot's belief in its current state, and should be conditioned on all available data. Thus we will define the robot's belief distribution to be:
\begin{equation} \label{eqBel}
\bel(x_t) = p(x_t \mathbin{\vert} z_{1:t})
\end{equation}

\subsection{Derivation}
Now we must figure out how to compute \(\bel(x_t)\). Let us start by using Bayes' Rule to rewrite \(\bel(x_t)\):
\begin{equation*}
\bel(x_t) = p(x_t \mathbin{\vert} z_{1:t}) = p(x_t \mathbin{\vert} z_t, z_{1:t-1}) = \frac{p(z_t \mathbin{\vert} x_t, z_{1:t-1})p(x_t \mathbin{\vert} z_{1:t-1})}{p(z_t \mathbin{\vert} z_{1:t-1})}
\end{equation*}

In order to continue, we will need to simplify \(p(z_t \mathbin{\vert} x_t, z_{1:t-1})\). Here we will make an important assumption. We will assume that the state \(x_t\) satisfies the Markov property: that is, \(x_t\) perfectly encapsulates all current and prior information \cite{robot_localization_paper}. Thus if \(x_t\) is known, then \(z_{1:t}\) are redundant, i.e. \(p(z_t \mathbin{\vert} x_t, z_{1:t}) = p(z_t \mathbin{\vert} x_t)\).

This assumption lets us remove consideration of past sensor measurements, and to rewrite the belief distribution as:
\begin{equation*}
\bel(x_t) = \frac{p(z_t \mathbin{\vert} x_t)p(x_t \mathbin{\vert} z_{1:t-1})}{p(z_t \mathbin{\vert} z_{1:t-1})}
\end{equation*}

Notice that \(p(z_t \mathbin{\vert} z_{1:t-1})\) is a constant with respect to \(x_t\). Thus it makes sense to define \(\eta = (p(z_t \mathbin{\vert} z_{1:t-1}))^{-1}\) and rewrite the belief distribution as:
\begin{equation*}
\bel(x_t) = \eta p(z_t \mathbin{\vert} x_t) p(x_t \mathbin{\vert} z_{1:t-1})
\end{equation*}
Notice that \(\bel(x_t)\) is a PDF, so it must integrate to 1. \(\eta\) acts as a normalizing constant enforcing this constraint.

Now \(\bel(x_t)\) has been split into two distributions of interest. Looking closely one may notice that \(p(x_t \mathbin{\vert} z_{1:t-1})\) is simply our original belief distribution, Eq. \ref{eqBel}, but not conditioned on the most recent sensor measurement \(z_t\). Let us refer to this distribution as \(\overline{\bel}(x_t)\), and break it down further using the \textit{Law of Total Probability} (Eq. \ref{eqTotalProb}) and our Markov assumption:
\begin{multline*}
\overline{\bel}(x_t) = p(x_t \mathbin{\vert} z_{1:t-1}) \\
= \int p(x_t \mathbin{\vert} x_{t-1}, z_{1:t-1}) p(x_{t-1} \mathbin{\vert} z_{1:t-1})dx_{t-1} \hfill \text{(by Eq. \ref{eqTotalProb})}\\
= \int p(x_t \mathbin{\vert} x_{t-1}) p(x_{t-1} \mathbin{\vert} z_{1:t-1})dx_{t-1} \hfill \text{  (by Markov assumption)} \\
= \int p(x_t \mathbin{\vert} x_{t-1}) \bel(x_{t-1})dx_{t-1}  \hfill \text{(by Eq. \ref{eqBel})}
\end{multline*}

We have arrived at a recursive definition of \(\bel(x_t)\) with respect to \(\bel(x_{t-1})\)! As long as \(p(x_t \mathbin{\vert} x_{t-1})\) and \(p(z_t \mathbin{\vert} x_t)\) are known, we can recursively calculate \(\bel(x_t)\) from some starting belief \(\bel(x_0)\).

\(p(x_t \mathbin{\vert} x_{t-1})\) defines a stochastic model for the robot's state, determining how the robot's state will evolve over time. This PDF will be referred to as the \textit{state transition probability}  \cite{probabilisticRobotics}.

\(p(z_t \mathbin{\vert} x_t)\) also defines a stochastic model, modeling the sensor measurements \(z_t\) as noisy projections of the robot's state. This PDF will be referred to as the \textit{measurement probability}  \cite{probabilisticRobotics}.

Once we assume the \textit{state transition probability} and \textit{measurement probability} PDFs are known, we can finally construct the algorithm known as Bayes' Filter:

\begin{algorithm} 
\caption{Bayes Filter} 
\label{alg:BayesFilter}
\begin{algorithmic}[1]
\Function{BayesFilterIterate}{ $\bel(x_{t-1})$, $z_t$ }
	\State{\(\overline{\bel}(x_t) = \int p(x_t \mathbin{\vert} x_{t-1}) \bel(x_{t-1})dx_{t-1} \)}
	\State{\(\bel(x_t) = \eta p(z_t \mathbin{\vert} x_t) \overline{\bel}(x_t) \)}
	\State{Set \(\int \bel(x_t)dx_t = 1\), and solve for \(\eta \)}
	\State{Use \(\eta\) to normalize \(\bel(x_t)\)}
	\State \Return{\(\bel(x_t)\)}
	\EndFunction
\end{algorithmic}
\end{algorithm}

%\subsection{Example}
\section{Extended Kalman Filter} \label{sectionEKF}

The most widely used algorithm implementing Bayes Filter is the Extended Kalman Filter (EKF). This filter approximates the PDFs found in Bayes Filter with multi-variate normal distributions. This class of PDFs are unimodal, and have the form 
\begin{equation} \label{eqDefNormal}
p(x) = N(x; E[X],\Sigma) = det(2 \pi \Sigma)^{-\frac{1}{2}} * exp \{-\frac{1}{2} (x - E[X])^T \Sigma^{-1} (x - E[X])\}
\end{equation}
where \(E[X]\) and \(\Sigma\) are the mean and covariance matrix of the random vector X, as described in section \ref{sectionProbTheory}. Notice that the mean and covariance matrix are sufficient to uniquely define any particular normal  distribution, and so the EKF only needs to keep track of a vector and matrix per PDF. So the belief distribution can be totally described by the best estimate for the state vector and the uncertainty in that estimate:
\begin{equation} \label{eqBelNormalDef}
\bel(x_t) = N(x_t; \mu_t,\Sigma_t)
\end{equation}
where \(\mu_t = E[X_t]\) is the mean or best state estimate, and \(\Sigma_t\) is the covariance matrix or uncertainty. These are the values which our new algorithm will try to compute; doing so will completely describe \(\bel(x_t)\).

Recall that Algorithm \ref{alg:BayesFilter} (Bayes' Filter) requires that the state transition PDF \(p(x_t \mathbin{\vert} x_{t-1})\) be given. So let us assume that the evolution of state is known via some model \[X_t =  g(X_{t-1}) + \epsilon_t\] where \(g: \mathbb{R}^n \to \mathbb{R}^n\) is some arbitrary function defining how the state evolves, and \(\epsilon_t\) is additive noise in the model. We will define \(\epsilon_t\) to be independent and identically distributed  random vectors of normal distribution, with mean 0 and covariance matrix \(R_t\).

% Why must we linearize?
If \(g\) is nonlinear, then it will distort the prior belief such that the posterior belief will no longer be normally distributed. In order to maintain our normal distribution assumption, we will have to make an approximation to the model. We will do so by linearizing \(g\) using a first-order Taylor series approximation around the current best estimate, \(\mu_{t-1}\).
\begin{multline*}
g(X_{t-1}) = g(\mu_{t-1}) + \frac{\partial g(\mu_{t-1})}{\partial X_{t-1}} (X_{t-1} - \mu_{t-1}) + O((X_{t-1} - \mu_{t-1})^2) \\
\approx g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1}) \hfill
\end{multline*}
where \[
G_t = \begin{bmatrix} 
\frac{\partial g_1(\mu_{t-1})}{\partial X_{1_{t-1}}} & \dots & \frac{\partial g_1(\mu_{t-1})}{\partial X_{n_{t-1}}}\\
\vdots & \ddots & \vdots & \\
\frac{\partial g_n(\mu_{t-1})}{\partial X_{1_{t-1}}} & \dots & \frac{\partial g_n(\mu_{t-1})}{\partial X_{n_{t-1}}} 
\end{bmatrix} \] is the Jacobian of \(g\) evaluated at \(\mu_{t-1}\). Then our approximate model is given by
\begin{equation*}
X_t = g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1}) + \epsilon_t
\end{equation*}
To describe the state transition PDF \(p(x_t \mathbin{\vert} x_{t-1})\), we are interested in finding its expectation and covariance matrix. We can do so directly from the approximate model \(X_t\) using the expectation operator introduced in Section \ref{sectionProbTheory}. Because the state transition PDF is conditioned on \(x_{t-1}\), we likewise will condition the expectation on \(X_{t-1}\). Then we can calculate the mean:
\begin{multline*}
E[X_t\ \mathbin{\vert} X_{t-1}] = E[g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1}) + \epsilon_t\ \mathbin{\vert} X_{t-1}] \\
= E[g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1})\ \mathbin{\vert} X_{t-1}] + E[\epsilon_t\ \mathbin{\vert} X_{t-1}] \hfill \text{(by linearity of E[X])} \\
= E[g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1})\ \mathbin{\vert} X_{t-1}] \hfill \text{(by the definition of \(\epsilon_t\))} \\
= g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1}) \hfill
\end{multline*}

and the covariance matrix:
\begin{multline*}
E[(X_t - E[X_t])(X_t - E[X_t])^T\ \mathbin{\vert} X_{t-1}] \\ = E[(g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1}) + \epsilon_t - g(\mu_{t-1}) - G_t (X_{t-1} - \mu_{t-1})) \\
* (g(\mu_{t-1}) + G_t (X_{t-1} - \mu_{t-1}) + \epsilon_t - g(\mu_{t-1}) - G_t (X_{t-1} - \mu_{t-1}))^T\ \mathbin{\vert} X_{t-1}] \\
= E[\epsilon_t \epsilon_t^T\ \mathbin{\vert} X_{t-1}] = E[(\epsilon_t - 0)(\epsilon_t - 0)^T\ \mathbin{\vert} X_{t-1}] \\
= E[(\epsilon_t - E[\epsilon_t\ \mathbin{\vert} X_{t-1}])(\epsilon_t - E[\epsilon_t\ \mathbin{\vert} X_{t-1}])^T\ \mathbin{\vert} X_{t-1}] \\
= R_t \hfill \text{(by the definition of \(\epsilon_t\))} \end{multline*}
and so the approximate Gaussian distribution describing the state transition PDF is
\begin{equation} \label{eqStateTransitionNormalDef}
p(x_t \mathbin{\vert} x_{t-1}) = N(x_t;g(\mu_{t-1}) + G_t (x_{t-1} - \mu_{t-1}),R_t)
\end{equation}

\subsection{Matrix Calculus}
% talk about matrix calculus and vector differentiation. (AB)^T = B^T A^T, (A+B)^T = A^T + B^T, if A is symmetric then so is A^-1, lemmas etc.
For the mathematical derivation of the Extended Kalman Filter which lies ahead, we will be working with matrix calculus.

For the benefit of the unfamiliar reader, here we will restate some elementary results which will be needed to follow along.

For \(A,B \in \mathbb{R}^{nxn}\):
\begin{gather}
A = \begin{pmatrix}
A_{11}  \dots A_{1n} \\
\vdots  \ddots  \vdots \\
A_{n1} \dots  A_{nn}
\end{pmatrix} \implies A^T = \begin{pmatrix}
A_{11}  \dots A_{n1} \\
\vdots  \ddots  \vdots  \\
A_{1n} \dots A_{nn}
\end{pmatrix} \\
(AB)^T = B^T A^T \\
(A+B)^T = A^T + B^T \\
A \text{ is symmetric} \implies A = A^T \\
A \text{ is symmetric and invertible } \implies A^{-1} \text{ is symmetric } 
\end{gather}

\begin{lemma} \label{lemma1}
	For \(x \in \mathbb{R} ^n \) and \(A \in \mathbb{R}^{nxn}\), A symmetric,
	\[\frac{\partial (x^T A x)}{\partial x} = 2 A x\]
\end{lemma}
\begin{proof}
	\begin{multline*}
	\text{Define } f: \mathbb{R}^n \to \mathbb{R}, f(x) = x^T A x \\
	x = \begin{pmatrix}
	x_1 \\
	\vdots \\
	x_n
	\end{pmatrix} \text{, }
	\frac{\partial{f}}{\partial{x}} = 
	\begin{pmatrix}
	\frac{\partial{f}}{\partial{x_1}} \\
	\vdots \\
	\frac{\partial{f}}{\partial{x_n}}
	\end{pmatrix} \text{, } e_i = 
	\begin{pmatrix}
	0 \\
	\vdots \\
	1 \\
	\vdots \\
	0
	\end{pmatrix} \\
	\text{For } h \in \mathbb{R} \text{,} \\
	\frac{\partial f}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x+h e_i) - f(x)}{h} = \lim_{h \rightarrow 0} \frac{(x + h e_i)^T A (x + h e_i) - x^T A x}{h} \\
	= \lim_{h \rightarrow 0} \frac{ x^T A x + h * x^T A e_i + h * e_i^T A x + h^2 * e_i^T A e_i - x^T A x}{h} \\
	= \lim_{h \rightarrow 0} \frac{ h [x^T A e_i + e_i^T A x + h * e_i^T A e_i]}{h} = \lim_{h \rightarrow 0} ( x^T A e_i + e_i^T A x + h * e_i^T A e_i) \\
	= x^T A e_i + e_i^T A x = e_i^T A^T x + e_i^T A x \hfill \text{(\(x^T A e_i\) is 1x1 \(\therefore x^T A e_i\) is symmetric)} \\
	= e_i^T ( A^T + A) x  = e_i^T ( A + A) x = e_i^T (2A x) \hfill \text{(by symmetry of A)} \\
	\text{Which tells us that the \(i^{th}\) component of \(\frac{\partial f}{\partial x} \) is equal to the \(i^{th}\) component of \(2Ax\).} \\
	\therefore \frac{\partial f}{\partial x} = 2Ax \hfill
	\end{multline*}
\end{proof}

\begin{lemma} \label{lemma2}
	For \(x,a \in \mathbb{R}^n \),
	\(\frac{\partial (x^T a)}{\partial x} = \frac{\partial (a^T x)}{\partial x} = a\)  
\end{lemma}
\begin{proof}
	\begin{multline*}
	\frac{\partial (a^T x)}{\partial x}  = \begin{pmatrix}
	\frac{\partial (a^T x)}{\partial x_1} \\
	\vdots \\
	\frac{\partial (A^T x)}{\partial x_n}
	\end{pmatrix} = \begin{pmatrix}
	\frac{\partial (a_1 x_1 + \dots + a_n x_n)}{\partial x_1} \\
	\vdots \\
	\frac{\partial (a_1 x_1 + \dots + a_n x_n)}{\partial x_n}
	\end{pmatrix}
	= \begin{pmatrix}
	a_1 \\
	\vdots \\
	a_n
	\end{pmatrix}
	= a \\
	a^T x \text{ has dimension 1 by 1, } \therefore a^T x = (a^T x)^T = x^T a \\
	\implies \frac{\partial (a^T x)}{\partial x} = \frac{\partial (x^T a)}{\partial x} = a \hfill
	\end{multline*}
\end{proof}

\begin{lemma} \label{lemmaTaylorDecomp}
Let \(f: \mathbb{R}^n \to \mathbb{R}\) be a quadratic function given by \[f(x) = x^T Ax + Bx + C\]
where \(A\) is a symmetric matrix of dimension \(n \times n\), \(B\) is a matrix of dimension \(1 \times n\), and \(C \in \mathbb{R}\). Then \(\exists x^* \in \mathbb{R}^n\) such that \(\frac{\partial f}{\partial x}(x^*) = 0\), and  \[
f(x) = f(x^*) + \frac{1}{2} (x-x^*)^T \frac{\partial^2 f}{\partial x^2} (x-x^*)
\] 

\normalfont {\textbf{Note:} This is a higher-dimensional application of Taylor's Theorem.}

\end{lemma}
\begin{proof}
\begin{multline*}
\frac{\partial f}{\partial x} = \frac{\partial (x^T A x)}{\partial x} + \frac{\partial (B x)}{\partial x} + \frac{\partial (C)}{\partial x} = 2Ax + B^T \hfill \text{(by prior Lemmas)} \\
\therefore \frac{\partial f}{\partial x} = 0 \implies x^* = -\frac{1}{2} A^{-1} B^T \\ \\
\frac{\partial^2 f}{\partial x^2} = \frac{\partial (2Ax)}{\partial x} + \frac{\partial (B^T)}{\partial x} = 2A^T = 2A \hfill \text{(by symmetry of A)} \\ \\
f(x^*) + \frac{1}{2} (x-x^*)^T \frac{\partial^2 f}{\partial x^2} (x-x^*) \\
= (-\frac{1}{2}A^{-1}B^T)^T A (-\frac{1}{2}A^{-1}B^T) + B(-\frac{1}{2}A^{-1}B^T) + C \\
+ \frac{1}{2}(x +\frac{1}{2}A^{-1}B^T)^T (2A) (x +\frac{1}{2}A^{-1}B^T) \\
= \frac{1}{4} B A^{-1} B^T - \frac{1}{2} B A^{-1} B^T + C + x^T A x + \frac{1}{2}Bx + \frac{1}{4} BA^{-1}B^T + \frac{1}{2}x^TB^T \\
= x^TAx + Bx + C = f(x) \hfill
\end{multline*}
\end{proof}

\begin{lemma} 
	\label{lemmaHessianInverse}
	Let \(p: \mathbb{R}^n \to [0,1]\) be a PDF given by \[
	p(x) = \eta_1\ exp\left\{ - L(x) \right\}
	\]
	where \(L: \mathbb{R}^n \to \mathbb{R}\), \(L(x) = x^T Ax + Bx + C\), and \(\eta_1\) is a normalizing constant. Then \(p\) defines a normal distribution with mean equal to the extremum of L, and covariance matrix equal to \((\frac{\partial^2 L}{\partial x})^{-1}\).
\end{lemma}
\begin{proof}
	Using Lemma \ref{lemmaTaylorDecomp}, we know that \[
	L(x) = L(a) + \frac{1}{2} (x-x^*)^T \frac{\partial^2 L}{\partial x^2} (x-x^*)
	\] where \(x^*\) is the extremum of \(L\).
	Therefore we can rewrite \(p\) as
	\begin{multline*}
	p(x) = \eta_1\ exp\Bigl\{ -L(a) - \frac{1}{2} (x-x^*)^T \frac{\partial^2 L}{\partial x^2} (x-x^*)\Bigr\} \\
	= \eta_1\ exp \Bigl\{ -L(a) \Bigr\} exp \Bigl\{ - \frac{1}{2} (x-x^*)^T \frac{\partial^2 L}{\partial x^2} (x-x^*)\Bigr\} \\
	= \eta_2\ exp \Bigl\{ - \frac{1}{2} (x-x^*)^T \frac{\partial^2 L}{\partial x^2} (x-x^*)\Bigr\} \hfill
	\end{multline*}
	Compare this form of \(p\) to the definition of the normal distribution given in Eq. \ref{eqDefNormal}. This is just a normal distribution, where \(E[X] = x^*\) and \(\Sigma^{-1} = \frac{\partial^2 L}{\partial x^2}\). Therefore the mean of \(p\) is the extremum of \(L\), and the covariance matrix of \(p\) is the inverse of the second derivative of \(L\). 
	%They share the same form, and since they are both PDFs, they integrate to 1., and therefore their normalizing constants \(\eta_2\) and \(\) are equal. They are 
\end{proof}

\begin{lemma}{\textbf{Inversion Lemma \cite{probabilisticRobotics}}} \label{lemmaInversion}
	
For \(R \in \mathbb{R}^{nxn}, Q \in \mathbb{R}^{kxk}, P \in \mathbb{R}^{nxk}\) where \(R\) and \(Q\) are invertible:
\[
(R + PQP^T)^{-1} = R^{-1} - R^{-1}P(Q^{-1} + P^T R^{-1}P)^{-1} P^T R^{-1}
\]
\end{lemma}
\begin{proof}
\begin{multline}
( R^{-1} - R^{-1}P(Q^{-1} + P^T R^{-1}P)^{-1} P^T R^{-1}) (R + PQP^T) \\
= [\underline{R^{-1}R}] + [R^{-1}PQP^T] - [R^{-1}P(Q^{-1} + P^T R^{-1}P)^{-1} P^T \underline{R^{-1}R}] \\
- [R^{-1}P(Q^{-1} + P^T R^{-1}P)^{-1} P^T R^{-1}PQP^T] \\
= [I] + [R^{-1}PQP^T] - [R^{-1}P(Q^{-1} + P^T R^{-1}P)^{-1} P^T] \\
- [R^{-1}P(Q^{-1} + P^T R^{-1}P)^{-1} P^T R^{-1}PQP^T] \\
= I + R^{-1}P [QP^T - (Q^{-1} + P^T R^{-1}P)^{-1} P^T \\
- (Q^{-1} + P^T R^{-1}P)^{-1} P^T R^{-1}PQP^T] \\
= I + R^{-1}P [QP^T - (Q^{-1} + P^T R^{-1}P)^{-1} \underline{Q^{-1} Q} P^T \\
- (Q^{-1} + P^T R^{-1}P)^{-1} P^T R^{-1}PQP^T] \\
= I + R^{-1}P [QP^T - \underline{(Q^{-1} + P^T R^{-1}P)^{-1} (Q^{-1} + P^T R^{-1}P)} Q P^T] \\
= I + R^{-1}P [\underline{QP^T - Q P^T}] = I \hfill
\end{multline}
\end{proof}


\subsection{Derivation}
Note that in the following mathematical derivation, because cov(X,Y) = cov(Y,X), the covariance matrix \(R_t\) is symmetric by definition.

We can now begin by rewriting Line 2 of Algorithm \ref{alg:BayesFilter} (the Bayes Filter) using our normal PDF definitions (Eqs. \ref{eqBelNormalDef} and \ref{eqStateTransitionNormalDef}):
\begin{multline} \label{eqBelBarLt}
\overline{\bel}(x_t) = \int p(x_t \mathbin{\vert} x_{t-1}) \bel(x_{t-1})dx_{t-1} \\
\approx det(2 \pi R_t)^{-\frac{1}{2}} det(2 \pi \Sigma_{t-1})^{-\frac{1}{2}} \int \Bigl[ exp\Bigl\{-\frac{1}{2} (x_t - g(\mu_{t-1}) - G_t (x_{t-1} - \mu_{t-1}))^T \\ * R_t^{-1} (x_t - g(\mu_{t-1}) - G_t (x_{t-1} - \mu_{t-1})) \Bigr\} * exp \Bigl\{-\frac{1}{2} (x_t - \mu_{t-1})^T \Sigma_{t-1}^{-1} (x_t - \mu_{t-1}) \Bigr\}dx_{t-1} \Bigr] \\
= \eta \int exp\Bigl\{-L_t\Bigr\}dx_{t-1} \hfill
\end{multline}
where the determinants are absorbed into a normalizing constant \(\eta\), and we have defined
\begin{multline} \label{eqDefLt}
L_t = \frac{1}{2} (x_t - g(\mu_{t-1}) - G_t (x_{t-1} - \mu_{t-1}))^T R_t^{-1} (x_t - g(\mu_{t-1}) - G_t (x_{t-1} - \mu_{t-1}))\\
+ \frac{1}{2} (x_t - \mu_{t-1})^T \Sigma_{t-1}^{-1} (x_t - \mu_{t-1}) \hfill
\end{multline}

By assumption, \(\overline{\bel}(x_t)\) is a normal distribution. We will define its mean and covariance to be \(\overline{\mu}\) and \(\overline{\Sigma}\) respectively. In order to compute these matrices, we would like to use Lemma \ref{lemmaHessianInverse}. However, to turn \(\overline{\bel}(x_t)\) into the proper form, we will have to get rid of the integral over \(x_{t-1}\). To do so, we will decompose \(L_t\) into two terms like so: \begin{equation} \label{eqLtDecomp}
L_t(x_{t-1},x_t) = M_t(x_{t-1},x_t) + P_t(x_t)
\end{equation}
where all terms containing \(x_{t-1}\) are collected in \(M_t\). This will allow us to move \(P_t\) outside of \(\overline{\bel}(x_t)\)'s integral, as it will not depend on \(x_{t-1}\). Then, as long as we choose our decomposition so that \(\int exp\Bigl\{ -M_t(x_{t-1},x_t)\Bigr\}dx_{t-1}\) does not depend on \(x_t\), we will be left with \(\overline{\bel}(x_t)\) in a form where we can apply Lemma \ref{lemmaHessianInverse}.

Now to proceed with this decomposition. Notice that \(L_t : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}\) is quadratic with respect to \(x_{t-1}\). By Lemma \ref{lemmaTaylorDecomp} we know that we can rewrite \(L_t\) using the extremum with respect to \(x_{t-1}: x_{t-1}^*\)
\begin{equation} \label{eqLtDecompUsingLemma}
L_t(x_{t-1},x_t) = L_t(x_{t-1}^*,x_t) + \frac{1}{2} (x_{t-1} - x_{t-1}^*) \frac{\partial^2 L_t}{\partial x_{t-1}^2} (x_{t-1} - x_{t-1}^*)
\end{equation}
Then we can let \[
P_t(x_t) = L_t(x_{t-1}^*,x_t) \\ M_t(x_{t-1},x_t) = \frac{1}{2} (x_{t-1} - x_{t-1}^*) \frac{\partial^2 L_t}{\partial x_{t-1}^2} (x_{t-1} - x_{t-1}^*)
\]
and we will have our desired decomposition in the form of Eq. \ref{eqLtDecomp}. To proceed with this decomposition, we need to calculate the extremum \(x_{t-1}^*\) and the second derivative \(\frac{\partial^2 L_t}{\partial x_{t-1}^2}\). To do so, we will first have to find \(\frac{\partial L_t}{\partial x_{t-1}}\). By expanding Eq. \ref{eqDefLt} into its quadratic and linear terms, and repeatedly applying Lemmas \ref{lemma1} and \ref{lemma2}, the result is calculated directly. This lengthy calculation is skipped here, and the final result is shown:
\begin{multline*}
\frac{\partial L_t}{\partial x_{t-1}} = -G_t^TR_t^{-1}(x_t - g(\mu_{t-1}) - G_t (x_{t-1} - \mu_{t-1})) + \Sigma_{t-1}^{-1}(x_{t-1} - \mu_{t-1}) \hfill
\end{multline*}
Then the second derivative is given by:
\begin{multline*}
\frac{\partial^2 L_t}{\partial x_{t-1}^2} = \frac{\partial (G_t^TR_t^{-1}G_tx_{t-1} + \Sigma_{t-1}^{-1}x_{t-1})}{\partial x_{t-1}} \\
= (G_t^TR_t^{-1}G_t)^T + (\Sigma_{t-1}^{-1})^T \hfill \text{(by Lemma \ref{lemma2})} \\
= G_t^TR_t^{-1}G_t + \Sigma_{t-1}^{-1} \equiv \Phi_t^{-1} \hfill \text{(by covariance matrix symmetry)}
\end{multline*}
Notice that we assume the second derivative is invertible. This will in fact always be true, since the operator is positive definite. Now we can set the first derivative of \(L_t\) to zero to compute its extremum:
\begin{multline}
\frac{\partial L_t}{\partial x_{t-1}} = 0 \\
\implies (x_{t-1}^* - \mu_{t-1})^T \Sigma_{t-1}^{-1} = (x_t - g(\mu_{t-1}) - G_t (x_{t-1}^* - \mu_{t-1}))^T R_t^{-1} G_t \\
\implies \Sigma_{t-1}^{-1} (x_{t-1}^* - \mu_{t-1}) = G_t^T R_t^{-1}  (x_t - g(\mu_{t-1}) - G_t (x_{t-1}^* - \mu_{t-1})) \\
\implies \Sigma_{t-1}^{-1} x_{t-1}^* + G_t^T R_t^{-1}G_t x_{t-1}^* = G_t^T R_t^{-1}  (x_t - g(\mu_{t-1}) + G_t \mu_{t-1}) + \Sigma_{t-1}^{-1} \mu_{t-1} \\
\implies \Phi_t^{-1} x_{t-1}^* = G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1} \\
\implies x_{t-1}^* = \Phi_t [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}] \hfill
\end{multline}
So our decomposition is given by:
\begin{multline*}
M_t(x_{t-1},x_t) = \frac{1}{2} (x_{t-1} - x_{t-1}^*)^T \frac{\partial^2 L_t}{\partial x_{t-1}^2} (x_{t-1} - x_{t-1}^*) \\
= \frac{1}{2} (x_{t-1} - \Phi_t [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}])^T \\
* \Phi_t^{-1} (x_{t-1} - \Phi_t [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}]) \hfill
\end{multline*}
and
\begin{multline} \label{eqDefPt}
P_t(x_t) = L_t(x_{t-1},x_t) -  M_t(x_{t-1},x_t) \\
= \frac{1}{2} (x_t - g(\mu_{t-1}) - G_t (x_{t-1} - \mu_{t-1}))^T R_t^{-1} (x_t - g(\mu_{t-1}) - G_t (x_{t-1} - \mu_{t-1}))\\
+ \frac{1}{2} (x_t - \mu_t)^T \Sigma_{t-1}^{-1} (x_t - \mu_t) - \frac{1}{2} (x_{t-1} - \Phi_t [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}])^T \\
* \Phi_t^{-1} (x_{t-1} - \Phi_t [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}]) \\
= \frac{1}{2} (x_t - g(\mu_{t-1}) + G_t \mu_{t-1})^T R_t^{-1} (x_t - g(\mu_{t-1}) + G_t \mu_{t-1}) + \frac{1}{2} \mu_{t-1}^T \Sigma_{t-1}^{-1} \mu_{t-1} \\
- \frac{1}{2} [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}]^T \Phi_t [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}] \hfill
\end{multline}
Notice that the quadratic and linear \(x_{t-1}\) terms canceled out in \(P_t\). This is to be expected from our decomposition, since \(P_t(x_t)\) should be constant with respect to \(x_{t-1}\). Also, comparing \(M_t(x_{t-1},x_t)\) to Eq. \ref{eqDefNormal}, we can see that it is the negative of the exponent in the \(N(x_{t-1};\Phi_t [G_t^T R_t^{-1}  (x_t - g(\mu_{t-1})) + \Phi_t^{-1} \mu_{t-1}], \Phi_t)\) distribution. And because all PDFs integrate to 1, we know that:
\begin{multline} \label{eqLtInt1}
\int det(2 \pi \Phi_t)^{-\frac{1}{2}} exp\Bigl\{-M_t(x_{t-1},x_t)\Bigr\}dx_{t-1} = 1 \\
\implies \int exp\Bigl\{-M_t(x_{t-1},x_t)\Bigr\}dx_{t-1} = det(2 \pi \Phi_t)^{\frac{1}{2}} \hfill
\end{multline}
Now we have all the pieces needed to further simplify \(\overline{\bel}(x_t)\).
\begin{multline} \label{belBarPartialDecomp}
\overline{\bel}(x_t) \approx \eta_1 \int exp\Bigl\{-L_t\Bigr\}dx_{t-1} \hfill \text{(by Eq. \ref{eqBelBarLt})} \\
= \eta_1 \int exp\Bigl\{-M_t(x_{t-1},x_t) - P_t(x_t)\Bigr\}dx_{t-1} \hfill \text{(by Eq. \ref{eqLtDecomp})} \\
= \eta_1 \int exp\Bigl\{-M_t(x_{t-1},x_t)\Bigr\} exp \Bigl\{ - P_t(x_t)\Bigr\}dx_{t-1} \\
= \eta_1\ exp \Bigl\{ - P_t(x_t)\Bigr\} \int exp\Bigl\{-M_t(x_{t-1},x_t)\Bigr\} dx_{t-1} \\
= \eta_1\ exp \Bigl\{ - P_t(x_t)\Bigr\} * det(2 \pi \Phi_t)^{\frac{1}{2}} \hfill \text{(by Eq. \ref{eqLtInt1})} \\
= \eta_2\ exp \Bigl\{ - P_t(x_t)\Bigr\} \hfill
\end{multline}

Notice that the \(det(2 \pi \Phi_t)^{\frac{1}{2}}\) term was absorbed into the normalizing constant \(\eta_2\), since it is a constant with respect to \(x_t\).

Because \(P_t(x_t)\) is quadratic, \(\overline{\bel}(x_t)\) now satisfies Lemma \ref{lemmaHessianInverse} (based on Eq. \ref{belBarPartialDecomp} and \ref{eqDefPt}). So we know that the mean and covariance of the normal distribution representing \(\overline{\bel}(x_t)\) will be equal to the extremum of \(P_t\), and the inverse of its second derivative. We will now compute these values.

\begin{multline*}
\frac{\partial P_t}{\partial x_t} = \frac{\partial [\frac{1}{2} x_t^T R_t^{-1}x_t]}{\partial x_t} + \frac{\partial [\frac{1}{2} x_t^TR_t^{-1}(-g(\mu_{t-1})+G_t\mu_{t-1})]}{\partial x_t} \\
+ \frac{\partial [\frac{1}{2} (-g(\mu_{t-1})+G_t\mu_{t-1})R_t^{-1}x_t]}{\partial x_t} + \frac{\partial [-\frac{1}{2} x_t^TR_t^{-1}G_t\Phi_tG_t^TR_t^{-1}x_t]}{\partial x_t} \\
+ \frac{\partial [-\frac{1}{2} x_t^TR_t^{-1}G_t\Phi_t(-G_t^TR_t^{-1}g_t + \Phi_t^{-1}\mu_{t-1})]}{\partial x_t} + \frac{\partial [-\frac{1}{2} (-G_t^TR_t^{-1}g_t + \Phi_t^{-1}\mu_{t-1})^T\Phi_tG_t^TR_t^{-1}x_t]}{\partial x_t} \\
= R_t^{-1}x_t - (R_t^{-1}G_t\Phi_tG_t^TR_t^{-1})^Tx_t  \hfill \text{(by Lemma \ref{lemma1})} \\
+ R_t^{-1}(-g(\mu_{t-1}) + G_t\mu_{t-1}) \hfill \text{(by Lemma \ref{lemma2})} \\
-R_t^{-1}G_t\Phi_t(-G_t^TR_t^{-1}g(\mu_{t-1} + \Phi_t^{-1}\mu_{t-1}) \hfill \text{(by Lemma \ref{lemma2})} \\
= (R_t^{-1} - R_t^{-1}G_t\Phi_tG_t^TR_t^{-1})x_t \\
+ R_t^{-1}(-g(\mu_{t-1}) + G_t\mu_{t-1}) - R_t^{-1}G_t\Phi_t(-G_t^TR_t^{-1}g(\mu_{t-1}) + \Phi_t^{-1}\mu_{t-1}) \\
= (R_t + G_t\Sigma_{t-1}G_t^T)^{-1}x_t \hfill \text{(by Lemma \ref{lemmaInversion})} \\
+ R_t^{-1}(-g(\mu_{t-1}) + G_t\mu_{t-1}) - R_t^{-1}G_t\Phi_t(-G_t^TR_t^{-1}g(\mu_{t-1}) + \Phi_t^{-1}\mu_{t-1}) \hfill
\end{multline*}

\begin{multline*}
\frac{\partial P_t}{\partial x_t} = 0 \\
\implies \overline{\mu_t} = (R_t + G_t\Sigma_{t-1}G_t^T) [R_t^{-1}G_t\Phi_t(-G_t^TR_t^{-1}g(\mu_{t-1}) + \Phi_t^{-1}\mu_{t-1}) \\ - R_t^{-1}(-g(\mu_{t-1}) + G_t\mu_{t-1})] \\
= (R_t + G_t\Sigma_{t-1}G_t^T) [-R_t^{-1}G_t\Phi_tG_t^TR_t^{-1}g(\mu_{t-1}) + R_t^{-1}G_t\mu_{t-1} \\ + R_t^{-1}g(\mu_{t-1}) - R_t^{-1}G_t\mu_{t-1}] \\
= (R_t + G_t\Sigma_{t-1}G_t^T) [(R_t^{-1} - R_t^{-1}G_t(G_t^TR_t^{-1}G_t + \Sigma_{t-1}^{-1})G_t^TR_t^{-1})g(\mu_{t-1}) \\
+ R_t^{-1}G_t\mu_{t-1} - R_t^{-1}G_t\mu_{t-1}] \\
= (R_t + G_t\Sigma_{t-1}G_t^T)[(R_t + G_t\Sigma_{t-1}G_t^T)^{-1} g(\mu_{t-1})] \\
= g(\mu_{t-1}) \hfill
\end{multline*}

\begin{multline*}
\frac{\partial^2 P_t }{\partial x_t^2} = \frac{(R_t + G_t\Sigma_{t-1}G_t^T)^{-1}x_t}{\partial x_t^2} \\
= [(R_t + G_t\Sigma_{t-1}G_t^T)^{-1}]^T = (R_t + G_t\Sigma_{t-1}G_t^T)^{-1} \\
\implies \overline{\Sigma_t} = R_t + G_t\Sigma_{t-1}G_t^T \hfill
\end{multline*}

Therefore \(\overline{\bel}(x_t) \approx N(x_t;g(\mu_{t-1}),(R_t + G_t\Sigma_tG_t^T))\). So Line 2 of the Bayes Filter, which propagated the belief forward in time based on the state transition PDF, can be translated into the following two computations:
\[
\overline{\mu_t} = g(\mu_{t-1}) \\
\overline{\Sigma_t} = (R_t + G_t\Sigma_{t-1}G_t^T)
\]
We are halfway done! The next line of Bayes filter is the update step, which updates the belief distribution based on the most recent sensor measurement:
\[
\bel(x_t) = \eta p(z_t \mathbin{\vert} x_t) \overline{\bel}(x_t)
\]

As before, let the underlying model of the measurement PDF be given by \[z_t =  h(x_t) + \delta_t\] where \(h: \mathbb{R}^n \to \mathbb{R}^k\) is an arbitrary function, and \(\delta_t\)'s are independent and identically distributed random vectors of normal distribution, with mean 0 and covariance matrix \(Q_t\).

Then we will once again use a first-order Taylor series expansion to approximate \(h\), only now we will expand around our new best estimate of the robot's state: \(\overline{\mu_t}\).
\begin{multline*}
h(x_t) = h(\overline{\mu_t}) + \frac{\partial h(\overline{\mu_t})}{\partial x_{t-1}} (x_t - \overline{\mu_t}) + O((x_t - \overline{\mu_t})^2) \\
\approx h(\overline{\mu_t}) + H_t (x_t - \overline{\mu_t}) \hfill
\end{multline*}
where \[
H_t = \begin{bmatrix} 
\frac{\partial h_1(\overline{\mu_t})}{\partial x_{1_{t-1}}} & \dots & \frac{\partial h_1(\overline{\mu_t})}{\partial x_{n_{t-1}}}\\
\vdots & \ddots & \vdots & \\
\frac{\partial h_k(\overline{\mu_t})}{\partial x_{1_{t-1}}} & \dots & \frac{\partial h_k(\overline{\mu_t})}{\partial x_{n_{t-1}}} 
\end{bmatrix} \] is the Jacobian of \(h\) evaluated at \(\overline{\mu_t}\).

Then in the same manner as we did with the state transition PDF, we can calculate the mean of the measurement PDF \(E[Z_t]\) to be \(h(\overline{\mu_t}) + H_t (X_t - \overline{\mu_t})\), with covariance matrix \(Q_t\). Thus the measurement PDF has the form \[p(z_t \mathbin{\vert} x_t)  \approx N(z_t;h(\overline{\mu_t}) + H_t (x_t - \overline{\mu_t}),Q_t)\] Note that just as before, the covariance matrix \(Q_t\) is symmetric by definition. Now we can rewrite the update step using the normal distribution definitions.
\begin{multline*}
\bel(x_t) = \eta_1\ p(z_t \mathbin{\vert} x_t) \overline{\bel}(x_t) \\
\approx \eta_1\ det(2 \pi Q_t)^{-\frac{1}{2}} det(2 \pi \overline{\Sigma_t})^{-\frac{1}{2}} exp\Bigl\{-\frac{1}{2} (z_t - h(\overline{\mu_t}) - H_t (x_t - \overline{\mu_t}))^T \\ * Q_t^{-1} (z_t - h(\overline{\mu_t}) - H_t (x_t - \overline{\mu_t}))\Bigr\} * exp \Bigl\{-\frac{1}{2} (x_t - \overline{\mu_t})^T \overline{\Sigma_t}^{-1} (x_t - \overline{\mu_t}) \Bigr\} \\
= \eta_2\ exp\bigl\{-J_t\bigr\} \hfill
\end{multline*} where \[
J_t = \frac{1}{2} (z_t - h(\overline{\mu_t}) - H_t (x_t - \overline{\mu_t}))^T Q_t^{-1} (z_t - h(\overline{\mu_t}) - H_t (x_t - \overline{\mu_t})) \\
+ \frac{1}{2} (x_t - \overline{\mu_t})^T \overline{\Sigma_t}^{-1} (x_t - \overline{\mu_t})
\]

\(\bel(x_t)\) must be normally distributed by assumption. We will denote its mean and covariance matrix by \(\mu\) and \(\Sigma\). This time there is no integral, and so we need not decompose \(J_t\). We can instead immediately apply Lemma \ref{lemmaHessianInverse}. 
Therefore the mean and covariance of the normal distribution representing \(\bel(x_t)\) will be equal to the extremum of \(J_t\), and the inverse of its second derivative. Let us now compute these values.

\begin{multline*}
\frac{\partial J_t}{\partial x_t} = \frac{\partial (\frac{1}{2} x_t^T )}{\partial x_t} - \frac{\partial (\frac{1}{2} x_t^T H_t^T Q_t^{-1} (z_t - h(\overline{\mu_t}) + H_t \overline{\mu_t}))}{\partial x_t} \\
- \frac{\partial (\frac{1}{2} (z_t - h(\overline{\mu_t}) + H_t \overline{\mu_t})^T Q_t^{-1} H_t x_t)}{\partial x_t} + \frac{\partial (\frac{1}{2} x_t^T \overline{\Sigma_t}^{-1} x_t)}{\partial x_t} \\
- \frac{\partial (\frac{1}{2} x_t^T \overline{\Sigma_t}^{-1} \overline{\mu_t})}{\partial x_t} - \frac{\partial (\frac{1}{2} \overline{\mu_t} \overline{\Sigma_t}^{-1} x_t )}{\partial x_t} \\
= H_t^T Q_t^{-1} H_t x_t - H_t^T Q_t^{-1} (z_t - h(\overline{\mu_t}) + H_t \overline{\mu_t}) \\
+ \overline{\Sigma_t}^{-1} x_t - \overline{\Sigma_t}^{-1} - \overline{\mu_t} \\
= (H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1}) (x_t - \overline{\mu_t}) + H_t^TQ_t^{-1}(h(\overline{\mu_t} - z_t) \hfill
\end{multline*}

\begin{multline*}
\frac{\partial^2 J_t}{\partial x_t^2} = H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1} \\
\therefore \Sigma_t = (H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1})^{-1} \hfill
\end{multline*}

\begin{multline*}
\frac{\partial J_t}{\partial x_t} = 0 \\
\implies (H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1}) (\mu_t - \overline{\mu_t}) = H_t^TQ_t^{-1}(z_t - h(\overline{\mu_t})) \\
\implies \Sigma_t^{-1} (\mu_t - \overline{\mu_t}) = H_t^TQ_t^{-1}(z_t - h(\overline{\mu_t})) \\
\implies \mu_t = \overline{\mu_t} + \Sigma_t H_t^TQ_t^{-1}(z_t - h(\overline{\mu_t})) \hfill
\end{multline*}

Thus the update step of the Bayes Filter, which updates the belief distribution using the most recent sensor measurement, can be translated into the following two computations:
\[
\mu_t = \overline{\mu_t} + \Sigma_t H_t^TQ_t^{-1}(z_t - h(\overline{\mu_t})) \\
\Sigma_t = (H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1})^{-1}
\]

% Why don't we have to normalize?
The only other steps of Bayes' Filter involve normalizing the belief distribution, which we need not perform since we already have the mean and covariance matrix of both \(\overline{\bel}(x_t)\) and \(\bel(x_t)\), which are the only pieces of information we need to fully describe their normal distributions. Therefore we can construct a new algorithm, the EKF.
\begin{algorithm} 
	\caption{Extended Kalman Filter}
	\label{alg:EKFnonOptimal}
	\begin{algorithmic}[1]
		\Function{EKF\_Iterate}{ $\mu_{t-1}$, $\Sigma_{t-1}$, $z_t$ }
		\State{\(\overline{\mu_t} = g(\mu_{t-1}) \)}
		\State{\(\overline{\Sigma_t} = (R_t + G_t\Sigma_{t-1}G_t^T)\)}
		\State{\(\Sigma_t = (H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1})^{-1} \)}
		\State{\(\mu_t = \overline{\mu_t} + \Sigma_t H_t^TQ_t^{-1}(z_t - h(\overline{\mu_t})) \)}
		\State \Return{\((\mu_t,\Sigma_t)\)}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

%% NEED TO JUSTIFY THIS, CALCULATE O() COMPLEXITY
We could stop here and be done with it. However, the computational complexity of inverting a \(d\) by \(d\) matrix using today's methods is approximately O(\(d^{2.8}\)). Line 4 of the current algorithm inverts the \(n\) by \(n\) matrix \(\overline{\Sigma_t}\), where \(n\) is the dimension of the state vector \(x_t\). So the computation of \(\Sigma_t\) given here is O(\(n^{2.8}\)). It turns out that we can rewrite Line 4 and avoid inverting the state covariance matrix as so:
\begin{multline*}
\Sigma_t = (H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1})^{-1} \\
= \overline{\Sigma_t} - \overline{\Sigma_t} H_t^T (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} H_t \overline{\Sigma_t} \hfill \text{(by Lemma \ref{lemmaInversion})} \\
= [ I - \overline{\Sigma_t} H_t^T (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} H_t ] \overline{\Sigma_t} \\
= [I - K_t H_t] \overline{\Sigma_t} \hfill
\end{multline*}
where we make the useful definition 
\[
K_t = \overline{\Sigma_t} H_t^T (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1}
\]
This shifts the O(\(n^{2.8}\)) computation of \(\Sigma_t\) to an O(\(k^{2.8}\)) computation of \(K_t\), where \(k\) is the dimension of the measurement vector \(z_t\). Note that in order to keep this new computation of \(\Sigma_t\) at O(\(n^2\)), one must multiply matrices in the proper order:
\[\Sigma_t = \overline{\Sigma_t} - K_t (H_t \overline{\Sigma_t})
\]
using the fact that the naive approach to multiplying two matrices of size \(n \times m\) and \(m \times o\) takes O(\(n m o\)) time.

For estimation problems involving state spaces of large dimensionality, \(n\) is often much larger than \(k\), and so this change leads to greater computational efficiency, which is important since we would like to run the EKF in real-time.

Lastly, note that
\begin{multline}
K_t = \overline{\Sigma_t} H_t^T (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} = (\Sigma_t \Sigma_t^{-1}) \overline{\Sigma_t} H_t^T (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} \\
= (\Sigma_t (H_t^T Q_t^{-1} H_t + \overline{\Sigma_t}^{-1})) \overline{\Sigma_t} H_t^T (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} \hfill \text{(by Line 4 of Algorithm \ref{alg:EKFnonOptimal})} \\
= \Sigma_t (H_t^T Q_t^{-1} H_t \overline{\Sigma_t} H_t^T + \overline{\Sigma_t}^{-1} \overline{\Sigma_t} H_t^T) (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} \\
= \Sigma_t (H_t^T Q_t^{-1} H_t \overline{\Sigma_t} H_t^T + H_t^T) (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} \\
= \Sigma_t (H_t^T Q_t^{-1} H_t \overline{\Sigma_t} H_t^T + H_t^T Q_t^{-1} Q_t) (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} \\
= \Sigma_t H_t^T Q_t^{-1} ( H_t \overline{\Sigma_t} H_t^T + Q_t) (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1} \\
= \Sigma_t H_t^T Q_t^{-1} \hfill
\end{multline}

which means the computation of \(\mu_t\) may also be rewritten using \(K_t\). Thus we can construct our more optimal version of the EKF.
\begin{algorithm} 
	\caption{Extended Kalman Filter}
	\label{alg:EKFOptimal}
	\begin{algorithmic}[1]
		\Function{EKF\_Iterate}{ $\mu_{t-1}$, $\Sigma_{t-1}$, $z_t$ }
		\State{\(\overline{\mu_t} = g(\mu_{t-1}) \)}
		\State{\(\overline{\Sigma_t} = (R_t + G_t\Sigma_{t-1}G_t^T)\)}
		\State{\(K_t = \overline{\Sigma_t} H_t^T (Q_t + H_t \overline{\Sigma_t} H_t^T)^{-1}\)}
		\State{\(\Sigma_t = (I - K_t H_t) \overline{\Sigma_t}\)}
		\State{\(\mu_t = \overline{\mu_t} + K_t(z_t - h(\overline{\mu_t}))\)}
		\State \Return{\((\mu_t,\Sigma_t)\)}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsection{Remarks}
In total the EKF algorithm has a computational complexity of \(max\)(O(\(n^{2.8}\)), O(\(k^{2.8}\)), O(\(g\)), O(\(h\))) per iteration, where the complexity of executing the functions \(g\) and \(h\) is unknown, but in practice are often O(\(1\)).

It is important to note that several assumptions made in the derivation are unlikely to be completely true in reality.

We assumed that all PDFs were represented by normal distributions, meaning they are unimodal. For many sensor measurements, this makes sense: the reported measurement is the unimodal mean of the measurement PDF, and the estimated noise of that measurement is encoded in the covariance matrix. However, in situations in which there are two or more equally likely states, the belief distribution is poorly approximated by a unimodal distribution.

We also used a first-order Taylor expansion to linearize the state transition and measurement models. If either \(g\) or \(h\) are extremely nonlinear, then the filter may experience large errors. However in practice, the EKF used in this project gave reasonable results in real-time. 

%All noise was assumed to be normally distributed with mean zero. If there are unmodeled system dynamics - as is often the case in the real world - then their effect can be encapsulated in state transition noise, as long as the effects are roughly linear


%markov assumption

%coming up with appropriate \(R_t\) and \(Q_t\) covariance matrices

%when does it diverge, when does it converge (unmodeled dynamics) \cite{}